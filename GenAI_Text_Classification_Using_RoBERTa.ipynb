{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Iqu4MzV53IzL",
        "outputId": "fe6172c3-e64e-470e-f1ea-4fb38d60fefe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "# Install library\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import library\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer, AutoConfig\n",
        "import numpy as np\n",
        "from scipy.special import softmax"
      ],
      "metadata": {
        "id": "NyyynRd64jkR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess text by replacing usernames and links with placeholders\n",
        "def preprocess(text):\n",
        "    return \" \".join(['@user' if word.startswith('@') and len(word) > 1 else 'http' if word.startswith('http') else word for word in text.split()])\n",
        "\n",
        "# Model and tokenizer initialization\n",
        "MODEL = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "config = AutoConfig.from_pretrained(MODEL)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
        "\n",
        "# Sample input text and preprocessing\n",
        "text = \"I had an amazing time at the concert! The energy was fantastic, and the performers were incredible.\"\n",
        "text = preprocess(text)\n",
        "\n",
        "# Tokenize and predict\n",
        "encoded_input = tokenizer(text, return_tensors='pt')\n",
        "output = model(**encoded_input)\n",
        "\n",
        "# Extract scores and apply softmax\n",
        "scores = softmax(output[0][0].detach().numpy())\n",
        "\n",
        "# Ranking labels based on scores\n",
        "ranking = np.argsort(scores)[::-1]\n",
        "\n",
        "# Display results\n",
        "for i, rank in enumerate(ranking):\n",
        "    label = config.id2label[rank]\n",
        "    score = np.round(float(scores[rank]), 4)\n",
        "    print(f\"{i + 1}) {label}: {score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzK98htf4pbB",
        "outputId": "f80efa53-284a-488e-91c8-f1d16732ab53"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1) positive: 0.9912\n",
            "2) neutral: 0.0055\n",
            "3) negative: 0.0033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess text by replacing usernames and links with placeholders\n",
        "def preprocess(text):\n",
        "    return \" \".join(['@user' if word.startswith('@') and len(word) > 1 else 'http' if word.startswith('http') else word for word in text.split()])\n",
        "\n",
        "# Model and tokenizer initialization\n",
        "MODEL = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "config = AutoConfig.from_pretrained(MODEL)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
        "\n",
        "# Sample input text and preprocessing\n",
        "text = \"The customer service was excellent, and they resolved my issue quickly. I couldnâ€™t be happier!\"\n",
        "text = preprocess(text)\n",
        "\n",
        "# Tokenize and predict\n",
        "encoded_input = tokenizer(text, return_tensors='pt')\n",
        "output = model(**encoded_input)\n",
        "\n",
        "# Extract scores and apply softmax\n",
        "scores = softmax(output[0][0].detach().numpy())\n",
        "\n",
        "# Ranking labels based on scores\n",
        "ranking = np.argsort(scores)[::-1]\n",
        "\n",
        "# Display results\n",
        "for i, rank in enumerate(ranking):\n",
        "    label = config.id2label[rank]\n",
        "    score = np.round(float(scores[rank]), 4)\n",
        "    print(f\"{i + 1}) {label}: {score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKTJRTAR5Tny",
        "outputId": "7c529a19-bb35-4efe-da4d-6750695f107f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1) positive: 0.9807\n",
            "2) neutral: 0.0128\n",
            "3) negative: 0.0064\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess text by replacing usernames and links with placeholders\n",
        "def preprocess(text):\n",
        "    return \" \".join(['@user' if word.startswith('@') and len(word) > 1 else 'http' if word.startswith('http') else word for word in text.split()])\n",
        "\n",
        "# Model and tokenizer initialization\n",
        "MODEL = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "config = AutoConfig.from_pretrained(MODEL)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
        "\n",
        "# Sample input text and preprocessing\n",
        "text = \"The food at the restaurant was delicious, and the atmosphere made the experience even better.\"\n",
        "text = preprocess(text)\n",
        "\n",
        "# Tokenize and predict\n",
        "encoded_input = tokenizer(text, return_tensors='pt')\n",
        "output = model(**encoded_input)\n",
        "\n",
        "# Extract scores and apply softmax\n",
        "scores = softmax(output[0][0].detach().numpy())\n",
        "\n",
        "# Ranking labels based on scores\n",
        "ranking = np.argsort(scores)[::-1]\n",
        "\n",
        "# Display results\n",
        "for i, rank in enumerate(ranking):\n",
        "    label = config.id2label[rank]\n",
        "    score = np.round(float(scores[rank]), 4)\n",
        "    print(f\"{i + 1}) {label}: {score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YIDfOpy5YKa",
        "outputId": "9284556b-02dc-4954-92e6-5600aa8c847e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1) positive: 0.9886\n",
            "2) neutral: 0.0085\n",
            "3) negative: 0.0028\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess text by replacing usernames and links with placeholders\n",
        "def preprocess(text):\n",
        "    return \" \".join(['@user' if word.startswith('@') and len(word) > 1 else 'http' if word.startswith('http') else word for word in text.split()])\n",
        "\n",
        "# Model and tokenizer initialization\n",
        "MODEL = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "config = AutoConfig.from_pretrained(MODEL)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
        "\n",
        "# Sample input text and preprocessing\n",
        "text = \"The service at the hotel was terrible. The staff was rude, and the room was dirty.\"\n",
        "text = preprocess(text)\n",
        "\n",
        "# Tokenize and predict\n",
        "encoded_input = tokenizer(text, return_tensors='pt')\n",
        "output = model(**encoded_input)\n",
        "\n",
        "# Extract scores and apply softmax\n",
        "scores = softmax(output[0][0].detach().numpy())\n",
        "\n",
        "# Ranking labels based on scores\n",
        "ranking = np.argsort(scores)[::-1]\n",
        "\n",
        "# Display results\n",
        "for i, rank in enumerate(ranking):\n",
        "    label = config.id2label[rank]\n",
        "    score = np.round(float(scores[rank]), 4)\n",
        "    print(f\"{i + 1}) {label}: {score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7iTxVEQ59bQ",
        "outputId": "4cc34a10-cea1-416a-9ca6-393735d7eb99"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1) negative: 0.9584\n",
            "2) neutral: 0.0368\n",
            "3) positive: 0.0048\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess text by replacing usernames and links with placeholders\n",
        "def preprocess(text):\n",
        "    return \" \".join(['@user' if word.startswith('@') and len(word) > 1 else 'http' if word.startswith('http') else word for word in text.split()])\n",
        "\n",
        "# Model and tokenizer initialization\n",
        "MODEL = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "config = AutoConfig.from_pretrained(MODEL)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
        "\n",
        "# Sample input text and preprocessing\n",
        "text = \"This movie was a complete waste of time. The plot was boring and predictable.\"\n",
        "text = preprocess(text)\n",
        "\n",
        "# Tokenize and predict\n",
        "encoded_input = tokenizer(text, return_tensors='pt')\n",
        "output = model(**encoded_input)\n",
        "\n",
        "# Extract scores and apply softmax\n",
        "scores = softmax(output[0][0].detach().numpy())\n",
        "\n",
        "# Ranking labels based on scores\n",
        "ranking = np.argsort(scores)[::-1]\n",
        "\n",
        "# Display results\n",
        "for i, rank in enumerate(ranking):\n",
        "    label = config.id2label[rank]\n",
        "    score = np.round(float(scores[rank]), 4)\n",
        "    print(f\"{i + 1}) {label}: {score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZqYJ4AN63kh",
        "outputId": "c6727649-5a9d-4399-8b98-0d85b57c79e5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1) negative: 0.9552\n",
            "2) neutral: 0.0391\n",
            "3) positive: 0.0057\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess text by replacing usernames and links with placeholders\n",
        "def preprocess(text):\n",
        "    return \" \".join(['@user' if word.startswith('@') and len(word) > 1 else 'http' if word.startswith('http') else word for word in text.split()])\n",
        "\n",
        "# Model and tokenizer initialization\n",
        "MODEL = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "config = AutoConfig.from_pretrained(MODEL)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
        "\n",
        "# Sample input text and preprocessing\n",
        "text = \"Iâ€™m really disappointed with this product. It stopped working after just one week.\"\n",
        "text = preprocess(text)\n",
        "\n",
        "# Tokenize and predict\n",
        "encoded_input = tokenizer(text, return_tensors='pt')\n",
        "output = model(**encoded_input)\n",
        "\n",
        "# Extract scores and apply softmax\n",
        "scores = softmax(output[0][0].detach().numpy())\n",
        "\n",
        "# Ranking labels based on scores\n",
        "ranking = np.argsort(scores)[::-1]\n",
        "\n",
        "# Display results\n",
        "for i, rank in enumerate(ranking):\n",
        "    label = config.id2label[rank]\n",
        "    score = np.round(float(scores[rank]), 4)\n",
        "    print(f\"{i + 1}) {label}: {score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyhxgyOR7QP7",
        "outputId": "541daef0-8611-43db-9f37-b61f0b2eb07f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1) negative: 0.9522\n",
            "2) neutral: 0.0423\n",
            "3) positive: 0.0055\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess text by replacing usernames and links with placeholders\n",
        "def preprocess(text):\n",
        "    return \" \".join(['@user' if word.startswith('@') and len(word) > 1 else 'http' if word.startswith('http') else word for word in text.split()])\n",
        "\n",
        "# Model and tokenizer initialization\n",
        "MODEL = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "config = AutoConfig.from_pretrained(MODEL)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
        "\n",
        "# Sample input text and preprocessing\n",
        "text = \"The meeting is scheduled for 3 PM, and the agenda will be shared beforehand.\"\n",
        "text = preprocess(text)\n",
        "\n",
        "# Tokenize and predict\n",
        "encoded_input = tokenizer(text, return_tensors='pt')\n",
        "output = model(**encoded_input)\n",
        "\n",
        "# Extract scores and apply softmax\n",
        "scores = softmax(output[0][0].detach().numpy())\n",
        "\n",
        "# Ranking labels based on scores\n",
        "ranking = np.argsort(scores)[::-1]\n",
        "\n",
        "# Display results\n",
        "for i, rank in enumerate(ranking):\n",
        "    label = config.id2label[rank]\n",
        "    score = np.round(float(scores[rank]), 4)\n",
        "    print(f\"{i + 1}) {label}: {score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVYOc5PU7VVr",
        "outputId": "9bd6c7fe-1d0e-449e-bd98-2fb7e1302480"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1) neutral: 0.9187\n",
            "2) positive: 0.0758\n",
            "3) negative: 0.0055\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess text by replacing usernames and links with placeholders\n",
        "def preprocess(text):\n",
        "    return \" \".join(['@user' if word.startswith('@') and len(word) > 1 else 'http' if word.startswith('http') else word for word in text.split()])\n",
        "\n",
        "# Model and tokenizer initialization\n",
        "MODEL = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "config = AutoConfig.from_pretrained(MODEL)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
        "\n",
        "# Sample input text and preprocessing\n",
        "text = \"He mentioned that the weather tomorrow is expected to be cloudy.\"\n",
        "text = preprocess(text)\n",
        "\n",
        "# Tokenize and predict\n",
        "encoded_input = tokenizer(text, return_tensors='pt')\n",
        "output = model(**encoded_input)\n",
        "\n",
        "# Extract scores and apply softmax\n",
        "scores = softmax(output[0][0].detach().numpy())\n",
        "\n",
        "# Ranking labels based on scores\n",
        "ranking = np.argsort(scores)[::-1]\n",
        "\n",
        "# Display results\n",
        "for i, rank in enumerate(ranking):\n",
        "    label = config.id2label[rank]\n",
        "    score = np.round(float(scores[rank]), 4)\n",
        "    print(f\"{i + 1}) {label}: {score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9e-beApf7z26",
        "outputId": "59d60d4d-2efc-4052-a39a-43e00d6a2908"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1) neutral: 0.8089\n",
            "2) negative: 0.1636\n",
            "3) positive: 0.0275\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess text by replacing usernames and links with placeholders\n",
        "def preprocess(text):\n",
        "    return \" \".join(['@user' if word.startswith('@') and len(word) > 1 else 'http' if word.startswith('http') else word for word in text.split()])\n",
        "\n",
        "# Model and tokenizer initialization\n",
        "MODEL = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "config = AutoConfig.from_pretrained(MODEL)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
        "\n",
        "# Sample input text and preprocessing\n",
        "text = \"The package was delivered to my house at noon.\"\n",
        "text = preprocess(text)\n",
        "\n",
        "# Tokenize and predict\n",
        "encoded_input = tokenizer(text, return_tensors='pt')\n",
        "output = model(**encoded_input)\n",
        "\n",
        "# Extract scores and apply softmax\n",
        "scores = softmax(output[0][0].detach().numpy())\n",
        "\n",
        "# Ranking labels based on scores\n",
        "ranking = np.argsort(scores)[::-1]\n",
        "\n",
        "# Display results\n",
        "for i, rank in enumerate(ranking):\n",
        "    label = config.id2label[rank]\n",
        "    score = np.round(float(scores[rank]), 4)\n",
        "    print(f\"{i + 1}) {label}: {score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zvRtkA576ej",
        "outputId": "ad4b187a-6fb8-4f25-ccde-4dccf217a165"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1) neutral: 0.7711\n",
            "2) positive: 0.2226\n",
            "3) negative: 0.0063\n"
          ]
        }
      ]
    }
  ]
}